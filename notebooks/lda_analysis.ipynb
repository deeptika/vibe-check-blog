{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf9277d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import LdaMulticore\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa1cc59-78b6-4d8c-b61a-c575260baa61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_number</th>\n",
       "      <th>content_type</th>\n",
       "      <th>network</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>public</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>comment</td>\n",
       "      <td>CBS</td>\n",
       "      <td>@Deano-n4w</td>\n",
       "      <td>Donald Trump's reasoning is lost üò¢</td>\n",
       "      <td>2024-08-27T09:51:01Z</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>comment</td>\n",
       "      <td>CBS</td>\n",
       "      <td>@noobpeepxpostyguitarists5381</td>\n",
       "      <td>2:05:31 this aged well</td>\n",
       "      <td>2024-08-26T09:48:00Z</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>comment</td>\n",
       "      <td>CBS</td>\n",
       "      <td>@noobpeepxpostyguitarists5381</td>\n",
       "      <td>The only facts is that words shouldn't be impo...</td>\n",
       "      <td>2024-08-26T09:44:19Z</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>comment</td>\n",
       "      <td>CBS</td>\n",
       "      <td>@sandhyagudigudi3998</td>\n",
       "      <td>Biden sir GodenBird flying ‚ù§Trumpsir like Baby...</td>\n",
       "      <td>2024-08-22T02:10:30Z</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>comment</td>\n",
       "      <td>CBS</td>\n",
       "      <td>@cherylcallahan5402</td>\n",
       "      <td>LIONEL NATION AND THE MATE ARE THE BEST BEST</td>\n",
       "      <td>2024-08-21T17:11:52Z</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195467</th>\n",
       "      <td>4</td>\n",
       "      <td>livechat</td>\n",
       "      <td>WFLA</td>\n",
       "      <td>@joannektc</td>\n",
       "      <td>I love that JD Vance was rocking the pink tie ...</td>\n",
       "      <td>2024-10-02T04:06:35Z</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195468</th>\n",
       "      <td>4</td>\n",
       "      <td>livechat</td>\n",
       "      <td>WFLA</td>\n",
       "      <td>@ChrisSchrum-t6y</td>\n",
       "      <td>Tampon Tim blows</td>\n",
       "      <td>2024-10-02T04:06:21Z</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195469</th>\n",
       "      <td>4</td>\n",
       "      <td>livechat</td>\n",
       "      <td>WFLA</td>\n",
       "      <td>@JygjjVdcc</td>\n",
       "      <td>But tims middle class</td>\n",
       "      <td>2024-10-02T04:05:20Z</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195470</th>\n",
       "      <td>4</td>\n",
       "      <td>livechat</td>\n",
       "      <td>WFLA</td>\n",
       "      <td>@NECKBRACEBRO</td>\n",
       "      <td>I felt like it was a tie.But I am still voting...</td>\n",
       "      <td>2024-10-02T04:04:56Z</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195471</th>\n",
       "      <td>4</td>\n",
       "      <td>livechat</td>\n",
       "      <td>WFLA</td>\n",
       "      <td>@nathandunlap7921</td>\n",
       "      <td>Bkowhard tim is lair</td>\n",
       "      <td>2024-10-02T04:04:12Z</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195472 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        debate_number content_type network                         author  \\\n",
       "0                   1      comment     CBS                     @Deano-n4w   \n",
       "1                   1      comment     CBS  @noobpeepxpostyguitarists5381   \n",
       "2                   1      comment     CBS  @noobpeepxpostyguitarists5381   \n",
       "3                   1      comment     CBS           @sandhyagudigudi3998   \n",
       "4                   1      comment     CBS            @cherylcallahan5402   \n",
       "...               ...          ...     ...                            ...   \n",
       "195467              4     livechat    WFLA                     @joannektc   \n",
       "195468              4     livechat    WFLA               @ChrisSchrum-t6y   \n",
       "195469              4     livechat    WFLA                     @JygjjVdcc   \n",
       "195470              4     livechat    WFLA                  @NECKBRACEBRO   \n",
       "195471              4     livechat    WFLA              @nathandunlap7921   \n",
       "\n",
       "                                                     text  \\\n",
       "0                      Donald Trump's reasoning is lost üò¢   \n",
       "1                                  2:05:31 this aged well   \n",
       "2       The only facts is that words shouldn't be impo...   \n",
       "3       Biden sir GodenBird flying ‚ù§Trumpsir like Baby...   \n",
       "4            LIONEL NATION AND THE MATE ARE THE BEST BEST   \n",
       "...                                                   ...   \n",
       "195467  I love that JD Vance was rocking the pink tie ...   \n",
       "195468                                   Tampon Tim blows   \n",
       "195469                              But tims middle class   \n",
       "195470  I felt like it was a tie.But I am still voting...   \n",
       "195471                               Bkowhard tim is lair   \n",
       "\n",
       "                published_at  like_count  public  \n",
       "0       2024-08-27T09:51:01Z           0    True  \n",
       "1       2024-08-26T09:48:00Z           0    True  \n",
       "2       2024-08-26T09:44:19Z           1    True  \n",
       "3       2024-08-22T02:10:30Z           0    True  \n",
       "4       2024-08-21T17:11:52Z           1    True  \n",
       "...                      ...         ...     ...  \n",
       "195467  2024-10-02T04:06:35Z         100    True  \n",
       "195468  2024-10-02T04:06:21Z           0    True  \n",
       "195469  2024-10-02T04:05:20Z           5    True  \n",
       "195470  2024-10-02T04:04:56Z          14    True  \n",
       "195471  2024-10-02T04:04:12Z          13    True  \n",
       "\n",
       "[195472 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/dataset_merged.csv', lineterminator='\\n')\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a678aa6-61d4-49dd-9db3-5260be2de1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "debate_number      0\n",
       "content_type       0\n",
       "network            0\n",
       "author           135\n",
       "text              13\n",
       "published_at       0\n",
       "like_count         0\n",
       "public             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c1b18d6-1b4c-47a0-8874-9282d5021f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_number</th>\n",
       "      <th>content_type</th>\n",
       "      <th>network</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>public</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [debate_number, content_type, network, author, text, published_at, like_count, public]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask1 = df['author'].isnull()\n",
    "mask2 = df['text'].isnull()\n",
    "\n",
    "df[mask1 & mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35d34b3-37ab-4d8b-a971-7c6729e28fae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debate_number</th>\n",
       "      <th>content_type</th>\n",
       "      <th>network</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>published_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>public</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193057</th>\n",
       "      <td>3</td>\n",
       "      <td>livechat</td>\n",
       "      <td>NBC</td>\n",
       "      <td>@steviet9485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-11T06:42:34Z</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        debate_number content_type network        author text  \\\n",
       "193057              3     livechat     NBC  @steviet9485  NaN   \n",
       "\n",
       "                published_at  like_count  public  \n",
       "193057  2024-09-11T06:42:34Z           0    True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = df[df['text'].isnull()]\n",
    "_[_['debate_number'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a0bf61-3847-4eee-8a50-61d2acc43eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/deeptikakannan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/deeptikakannan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/deeptikakannan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deeptikakannan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk downloads and setup\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "more_stopwords = ['www.youtube.com', 'https', 'http', 'www.youtube.com/watch']\n",
    "stop_words = set(list(stopwords.words('english')) + more_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e033b3e4-50a4-4907-8d5c-01f2ff17493d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:7: DeprecationWarning: invalid escape sequence '\\.'\n",
      "/scratch/local/49802054/ipykernel_294736/2135773304.py:7: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  df['processed_text'] = df['text'].apply(lambda x: re.sub('[,\\.!?]', '', x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump's reasoning is lost üò¢</td>\n",
       "      <td>donald trump's reasoning is lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2:05:31 this aged well</td>\n",
       "      <td>2:05:31 this aged well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The only facts is that words shouldn't be impo...</td>\n",
       "      <td>the only facts is that words shouldn't be impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biden sir GodenBird flying ‚ù§Trumpsir like Baby...</td>\n",
       "      <td>biden sir godenbird flying trumpsir like baby ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIONEL NATION AND THE MATE ARE THE BEST BEST</td>\n",
       "      <td>lionel nation and the mate are the best best</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                 Donald Trump's reasoning is lost üò¢   \n",
       "1                             2:05:31 this aged well   \n",
       "2  The only facts is that words shouldn't be impo...   \n",
       "3  Biden sir GodenBird flying ‚ù§Trumpsir like Baby...   \n",
       "4       LIONEL NATION AND THE MATE ARE THE BEST BEST   \n",
       "\n",
       "                                      processed_text  \n",
       "0                  donald trump's reasoning is lost   \n",
       "1                             2:05:31 this aged well  \n",
       "2  the only facts is that words shouldn't be impo...  \n",
       "3  biden sir godenbird flying trumpsir like baby ...  \n",
       "4       lionel nation and the mate are the best best  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing data\n",
    "\n",
    "df = df.drop(columns=['debate_number', 'content_type', 'network', 'author', 'published_at', 'like_count', 'public'], axis=1)\n",
    "\n",
    "df['text'] = df['text'].apply(str)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "df['processed_text'] = df['processed_text'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "\n",
    "df['processed_text'] = df['processed_text'].apply(lambda x: x.lower())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1a373b-f9cd-4068-b72f-d23c20854d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['donald', 'trump', 'reasoning', 'is', 'lost'], ['this', 'aged', 'well']]\n"
     ]
    }
   ],
   "source": [
    "# tokenize text\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = df.processed_text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:5][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1990ad-5b9b-44e4-bf06-fedf3740112d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['donald', 'trump', 'reasoning', 'lost'], ['aged', 'well']]\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# bigram, trigram model buildings\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) \n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# lemmatize keeping only noun, adj, vb, adv\n",
    "def lemmatize(texts, allowed_postags=['NN', 'JJ', 'VB', 'RB']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        pos_tags = nltk.pos_tag(sent)\n",
    "        texts_out.append([lemmatizer.lemmatize(word) for word, pos in pos_tags if pos[:2] in allowed_postags])\n",
    "    return texts_out  \n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "data_lemmatized = lemmatize(data_words_bigrams, allowed_postags=['NN', 'JJ', 'VB', 'RB'])\n",
    "\n",
    "print(data_lemmatized[:5][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219be0fa-ceb0-44f6-8810-4f2bffa8bb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus and dictionary\n",
    "\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "corpus[:5][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edebbb7-2b34-4202-89af-d18560f6ab34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LDA model\n",
    "\n",
    "# number of topics - can change - TODO\n",
    "num_topics = 10\n",
    "\n",
    "# LDA model\n",
    "\n",
    "# load pre-saved model\n",
    "# lda_model = models.ldamodel.LdaModel.load('lda_model_full_dataset.model')\n",
    "\n",
    "# train model\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, random_state=42, passes=10, workers=4, chunksize=100, per_word_topics=True)\n",
    "# lda_model.save('lda_viz/full_dataset_10_topics.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e9298b-e5b7-4818-a615-aea4bd169052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.034*\"cnn\" + 0.029*\"debate\" + 0.029*\"abc\" + 0.028*\"moderator\" + '\n",
      "  '0.027*\"number\" + 0.020*\"ever\" + 0.017*\"seems\" + 0.016*\"job\" + 0.014*\"seen\" '\n",
      "  '+ 0.013*\"news\"'),\n",
      " (1,\n",
      "  '0.099*\"debate\" + 0.057*\"question\" + 0.029*\"answer\" + 0.024*\"time\" + '\n",
      "  '0.020*\"lol\" + 0.014*\"presidential\" + 0.014*\"even\" + 0.014*\"candidate\" + '\n",
      "  '0.011*\"minute\" + 0.011*\"first\"'),\n",
      " (2,\n",
      "  '0.289*\"trump\" + 0.053*\"biden\" + 0.031*\"donald\" + 0.025*\"love\" + '\n",
      "  '0.019*\"joke\" + 0.014*\"talking\" + 0.013*\"understand\" + 0.012*\"saying\" + '\n",
      "  '0.012*\"stand\" + 0.012*\"went\"'),\n",
      " (3,\n",
      "  '0.029*\"people\" + 0.011*\"war\" + 0.010*\"country\" + 0.010*\"want\" + 0.010*\"get\" '\n",
      "  '+ 0.010*\"american\" + 0.009*\"money\" + 0.009*\"year\" + 0.009*\"care\" + '\n",
      "  '0.009*\"comment\"'),\n",
      " (4,\n",
      "  '0.108*\"biden\" + 0.089*\"joe\" + 0.050*\"kamala\" + 0.045*\"harris\" + '\n",
      "  '0.019*\"watch\" + 0.017*\"lost\" + 0.016*\"sound\" + 0.015*\"bad\" + 0.015*\"look\" + '\n",
      "  '0.014*\"start\"'),\n",
      " (5,\n",
      "  '0.059*\"biden\" + 0.057*\"lie\" + 0.038*\"look\" + 0.017*\"liar\" + 0.016*\"lying\" + '\n",
      "  '0.015*\"face\" + 0.011*\"border\" + 0.010*\"see\" + 0.009*\"bidens\" + '\n",
      "  '0.009*\"retire\"'),\n",
      " (6,\n",
      "  '0.078*\"trump\" + 0.064*\"biden\" + 0.037*\"let\" + 0.035*\"go\" + 0.027*\"say\" + '\n",
      "  '0.023*\"talk\" + 0.022*\"vote\" + 0.020*\"need\" + 0.019*\"time\" + 0.017*\"get\"'),\n",
      " (7,\n",
      "  '0.071*\"old\" + 0.069*\"man\" + 0.029*\"show\" + 0.025*\"year\" + 0.021*\"child\" + '\n",
      "  '0.018*\"guy\" + 0.016*\"men\" + 0.016*\"embarrassing\" + 0.013*\"end\" + '\n",
      "  '0.012*\"kid\"'),\n",
      " (8,\n",
      "  '0.026*\"god\" + 0.021*\"abortion\" + 0.020*\"woman\" + 0.015*\"help\" + '\n",
      "  '0.013*\"cant\" + 0.013*\"usa\" + 0.012*\"israel\" + 0.011*\"hope\" + 0.011*\"find\" + '\n",
      "  '0.010*\"biased\"'),\n",
      " (9,\n",
      "  '0.050*\"trump\" + 0.047*\"president\" + 0.036*\"biden\" + 0.020*\"america\" + '\n",
      "  '0.020*\"country\" + 0.016*\"american\" + 0.016*\"people\" + 0.015*\"harris\" + '\n",
      "  '0.013*\"think\" + 0.013*\"world\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d98b840-9659-4c3a-85c6-1322dff4117f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved\n"
     ]
    }
   ],
   "source": [
    "# visualize initial model\n",
    "\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, 'lda_viz/vis_full_dataset_initial_model.html')\n",
    "print(\"Visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301272cc-a888-4099-babe-12acdaf8a926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.5628066643308023\n"
     ]
    }
   ],
   "source": [
    "# coherence score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "print('Coherence Score: ', coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395c946b-9776-424b-a02c-05b2a04af888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98167, 8) (86131, 8) (10132, 8) (1042, 8)\n"
     ]
    }
   ],
   "source": [
    "# train model over each debate comments/livechat\n",
    "\n",
    "# partition dataset\n",
    "\n",
    "df = pd.read_csv('../Election-Data/dataset_merged.csv', lineterminator='\\n')\n",
    "\n",
    "debate_1 = df[df['debate_number'] == 1]\n",
    "debate_2 = df[df['debate_number'] == 2]\n",
    "debate_3 = df[df['debate_number'] == 3]\n",
    "debate_4 = df[df['debate_number'] == 4]\n",
    "\n",
    "print(debate_1.shape, debate_2.shape, debate_3.shape, debate_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b062f9-61a3-4adb-8a9a-8f55dff9fc41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: DeprecationWarning: invalid escape sequence '\\.'\n",
      "<>:9: DeprecationWarning: invalid escape sequence '\\.'\n",
      "/scratch/local/49802054/ipykernel_294736/1817291725.py:9: DeprecationWarning: invalid escape sequence '\\.'\n",
      "  df['processed_text'] = df['text'].apply(lambda x: re.sub('[,\\.!?]', '', x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on 2020 Presidential Debate: Biden vs Trump\n",
      "Text preprocessed\n",
      "Tokenized\n",
      "LDA model trained\n",
      "Visualization saved\n",
      "Coherence Score for 2020 Presidential Debate: Biden vs Trump is: 0.5280143302306499\n",
      "\n",
      "Starting on 2024 Presidential Debate 1: Biden vs Trump\n",
      "Text preprocessed\n",
      "Tokenized\n",
      "Lemmatized\n",
      "Corpus & dictionary generated\n",
      "LDA model trained\n",
      "Visualization saved\n",
      "Coherence Score for 2024 Presidential Debate 1: Biden vs Trump is: 0.5282959166998478\n",
      "\n",
      "Starting on 2024 Presidential Debate 2: Harris vs Trump\n",
      "Text preprocessed\n",
      "Tokenized\n",
      "Lemmatized\n",
      "Corpus & dictionary generated\n",
      "LDA model trained\n",
      "Visualization saved\n",
      "Coherence Score for 2024 Presidential Debate 2: Harris vs Trump is: 0.4857249073638247\n",
      "\n",
      "Starting on 2024 Vice Presidential Debate: Vance vs Walz\n",
      "Text preprocessed\n",
      "Tokenized\n",
      "Lemmatized\n",
      "Corpus & dictionary generated\n",
      "LDA model trained\n",
      "Visualization saved\n",
      "Coherence Score for 2024 Vice Presidential Debate: Vance vs Walz is: 0.3641264606396487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train per debate\n",
    "\n",
    "def preprocess_train_validate_visualize(debate_name, df):\n",
    "    print('Starting on {}'.format(debate_name))\n",
    "    \n",
    "    # preprocess text\n",
    "    df = df.drop(columns=['debate_number', 'content_type', 'network', 'author', 'published_at', 'like_count', 'public'], axis=1)\n",
    "    df['text'] = df['text'].apply(str)\n",
    "    df['processed_text'] = df['text'].apply(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "    df['processed_text'] = df['processed_text'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "    df['processed_text'] = df['processed_text'].apply(lambda x: x.lower())\n",
    "    print('Text preprocessed')\n",
    "    \n",
    "    # tokenize\n",
    "    data = df.processed_text.values.tolist()\n",
    "    data_words = list(sent_to_words(data))\n",
    "    print('Tokenized')\n",
    "    \n",
    "    # lemmatize\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    data_lemmatized = lemmatize(data_words_bigrams, allowed_postags=['NN', 'JJ', 'VB', 'RB'])\n",
    "    print('Lemmatized')\n",
    "    \n",
    "    # corpus and dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    texts = data_lemmatized\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    print('Corpus & dictionary generated')\n",
    "    \n",
    "    # train LDA model\n",
    "    lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=10, random_state=42, passes=10, workers=4, chunksize=100, per_word_topics=True)\n",
    "    print('LDA model trained')\n",
    "    \n",
    "    # visualize\n",
    "    vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    pyLDAvis.save_html(vis, 'lda_viz/vis_{}_initial_model.html'.format(debate_name))\n",
    "    print('Visualization saved')\n",
    "    \n",
    "    # calculate coherence\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    print('Coherence Score for {} is: {}'.format(debate_name, coherence_model_lda.get_coherence()))\n",
    "    print()\n",
    "\n",
    "debates = {'2020 Presidential Debate: Biden vs Trump': debate_1,\n",
    "           '2024 Presidential Debate 1: Biden vs Trump': debate_2,\n",
    "           '2024 Presidential Debate 2: Harris vs Trump': debate_3,\n",
    "           '2024 Vice Presidential Debate: Vance vs Walz': debate_4}\n",
    "\n",
    "for debate_name, df in debates.items():\n",
    "    preprocess_train_validate_visualize(debate_name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a95f6-bbde-41b6-a839-d0167f246c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/48 [02:28<1:55:57, 148.04s/it]"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning for the full dataset\n",
    "\n",
    "import tqdm\n",
    "\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           workers=3)  # Adjust 'workers' depending on your machine cores.\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# alpha param\n",
    "alpha = list(np.arange(0.01, 1, 0.5))  # Fewer steps\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# beta param\n",
    "beta = list(np.arange(0.01, 1, 0.5))  # Fewer steps\n",
    "beta.append('symmetric')\n",
    "\n",
    "# topics range\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 2\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# single corpus sample\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sample = gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75))\n",
    "\n",
    "model_results = {'Topics': [], 'Alpha': [], 'Beta': [], 'Coherence': []}\n",
    "\n",
    "pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)))\n",
    "\n",
    "# iterate over different combinations\n",
    "for k in topics_range:\n",
    "    for a in alpha:\n",
    "        for b in beta:\n",
    "            cv = compute_coherence_values(corpus=corpus_sample, dictionary=id2word, k=k, a=a, b=b)\n",
    "            model_results['Topics'].append(k)\n",
    "            model_results['Alpha'].append(a)\n",
    "            model_results['Beta'].append(b)\n",
    "            model_results['Coherence'].append(cv)\n",
    "            pbar.update(1)\n",
    "\n",
    "pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cffd8b-0070-4f09-b6af-9b714eef89be",
   "metadata": {},
   "source": [
    "TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade84bb3-ee14-4f32-8628-e300830b5249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lda_model.print_topics(), columns=['topic', 'words & their probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137dfdd-36d7-4346-8cdd-cadec0eb4663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model testing - coherence score\n",
    "# measures how interpretable or meaningful the topics are; a higher score indicates better topic quality\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df['processed_text'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56f5bc-1904-43d2-84db-083e29de9e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# varying number of topics\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
    "    num_topics_values = range(start, limit, step)\n",
    "    coherence_values = []\n",
    "    for n_topic in num_topics_values:\n",
    "        print('Number of Topics: {}'.format(n_topic))\n",
    "        \n",
    "        # training model\n",
    "        model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=n_topic, \n",
    "                             random_state=42, passes=10, workers=4)   \n",
    "        print('LDA Model Trained')\n",
    "        \n",
    "        # calculating coherence\n",
    "        coherencemodel = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        _ = coherencemodel.get_coherence()\n",
    "        print('Coherence value: {}'.format(_))\n",
    "        coherence_values.append(_)\n",
    "    \n",
    "    return num_topics_values, coherence_values\n",
    "\n",
    "start = 2\n",
    "limit = 40\n",
    "step = 2\n",
    "\n",
    "num_topics_values, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, \n",
    "                                                        texts=df['processed_text'], start=start, \n",
    "                                                        limit=limit, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc387eac-2fa7-462e-9778-243a6a5867cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize results\n",
    "\n",
    "plt.plot(num_topics_values, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.title(\"Coherence Scores by Number of Topics\")\n",
    "plt.show()\n",
    "\n",
    "# no change in coherence score with increasing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bb01b-d1af-4c68-b468-5258d04092b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# analysis per debate\n",
    "\n",
    "debates = ['2020 Presidential Debate: Biden vs Trump', \n",
    "           '2024 Presidential Debate 1: Biden vs Trump',\n",
    "           '2024 Presidential Debate 2: Harris vs Trump',\n",
    "           '2024 Vice-presidential Debate 3: Vance vs Walz']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
